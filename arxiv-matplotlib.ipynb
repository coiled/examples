{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ae977bc3-d3cf-4492-a359-a95f8156fb52",
   "metadata": {},
   "source": [
    "<img src=\"https://github.com/mrocklin/arxiv-matplotlib/raw/main/results.png?raw=true\"\n",
    "     align=\"right\"\n",
    "     width=\"50%\"/>\n",
    "\n",
    "# How Popular is Matplotlib?\n",
    "\n",
    "Anecdotally the Matplotlib maintainers were told \n",
    "\n",
    "*\"About 15% of arXiv papers use Matplotlib\"*\n",
    "\n",
    "arXiv is the preeminent repository for scholarly prepreint articles.  It stores millions of journal articles used across science.  It's also public access, and so we can just scrape the entire thing given enough compute power.\n",
    "\n",
    "## Watermark\n",
    "\n",
    "Starting in the early 2010s, Matplotlib started including the bytes `b\"Matplotlib\"` in every PNG and PDF that they produce.  These bytes persist in PDFs that contain Matplotlib plots, including the PDFs stored on arXiv.  As a result, it's pretty simple to check if a PDF contains a Matplotlib image.  All we have to do is scan through every PDF and look for these bytes; no parsing required.\n",
    "\n",
    "## Data\n",
    "\n",
    "The data is stored in a requester pays bucket at s3://arxiv (more information at https://arxiv.org/help/bulk_data_s3 ) and also on GCS hosted by Kaggle (more information at https://www.kaggle.com/datasets/Cornell-University/arxiv).  \n",
    "\n",
    "The data is about 1TB in size.  We're going to use Dask for this.\n",
    "\n",
    "This is a good example of writing plain vanilla Python code to solve a problem, running into issues of scale, and then using Dask to easily jump over those problems."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f0965a0-fa87-470b-bd3d-0b5b7ecaca99",
   "metadata": {},
   "source": [
    "### Get all filenames\n",
    "\n",
    "Our data is stored in a requester pays S3 bucket in the `us-east-1` region.  Each file is a tar file which contains a directory of papers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e62539ef-5e91-43c5-afa8-0c3fa51b8f11",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import s3fs\n",
    "s3 = s3fs.S3FileSystem(requester_pays=True)\n",
    "\n",
    "directories = s3.ls(\"s3://arxiv/pdf\")\n",
    "directories[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e5cb2b5-1ad5-4a21-b98d-4f0f615dacd6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "len(directories)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92438ddf-7b02-462d-8a5d-2b2e760dd1a4",
   "metadata": {},
   "source": [
    "There are lots of these"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1646a64-e4b2-4965-98d8-d0d5322e4368",
   "metadata": {},
   "outputs": [],
   "source": [
    "s3.du(\"s3://arxiv/pdf\") / 1e12"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4d3219f-a6ce-487f-b5ae-522a7415c014",
   "metadata": {},
   "source": [
    "## Process one file with plain Python\n",
    "\n",
    "Mostly we have to muck about with tar files.  This wasn't hard.  The `tarfile` library is in the stardard library.  It's not beautiful, but it's also not hard to use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85146f13-5e5a-40e3-8d56-a79064f35ce4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import tarfile\n",
    "import io\n",
    "\n",
    "def extract(filename: str):\n",
    "    \"\"\" Extract and process one directory of arXiv data\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    filename: str\n",
    "    contains_matplotlib: boolean\n",
    "    \"\"\"\n",
    "    out = []\n",
    "    with s3.open(filename) as f:\n",
    "        bytes = f.read()\n",
    "        with io.BytesIO() as bio:\n",
    "            bio.write(bytes)\n",
    "            bio.seek(0)\n",
    "            try:\n",
    "                with tarfile.TarFile(fileobj=bio) as tf:\n",
    "                    for member in tf.getmembers():\n",
    "                        if member.isfile() and member.name.endswith(\".pdf\"):\n",
    "                            data = tf.extractfile(member).read()\n",
    "                            out.append((\n",
    "                                member.name, \n",
    "                                b\"matplotlib\" in data.lower()\n",
    "                            ))\n",
    "            except tarfile.ReadError:\n",
    "                pass\n",
    "            return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de243ca8-bcd2-47b4-8574-bce3f0bda790",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# See an example of its use\n",
    "extract(directories[20])[:20]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9feae86b-4c46-455d-8e5b-09eb80ec3400",
   "metadata": {},
   "source": [
    "# Scale processing to full dataset\n",
    "\n",
    "Great, we can get a record of each file and whether or not it used Matplotlib.  Each of these takes about a minute to run on my local machine.  Processing all 5000 files would take 5000 minutes, or around 100 hours.  \n",
    "\n",
    "We can accelerate this in two ways:\n",
    "\n",
    "1.  **Process closer to the data** by spinning up resources in the same region on the cloud (this also reduces data transfer costs)\n",
    "2.  **Use hundreds of workers** in parallel\n",
    "\n",
    "We can do this easily with [Coiled Functions](https://docs.coiled.io/user_guide/usage/functions/index.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34748fc3-7f2f-442e-9829-626d88878234",
   "metadata": {},
   "source": [
    "## Run function on the cloud in parallel\n",
    "\n",
    "We annotate our `extract` function with the `@coiled.function` decorator to have it run on AWS in the same region where the data is stored."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "245c269a-c432-4352-aa01-0d9110b63304",
   "metadata": {},
   "outputs": [],
   "source": [
    "import coiled\n",
    "\n",
    "@coiled.function(\n",
    "    region=\"us-east-1\",  # Local to data.  Faster and cheaper.\n",
    "    vm_type=\"m6i.xlarge\",\n",
    "    threads_per_worker=4,\n",
    ")\n",
    "def extract(filename: str):\n",
    "    \"\"\" Extract and process one directory of arXiv data\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    filename: str\n",
    "    contains_matplotlib: boolean\n",
    "    \"\"\"\n",
    "    out = []\n",
    "    with s3.open(filename) as f:\n",
    "        bytes = f.read()\n",
    "        with io.BytesIO() as bio:\n",
    "            bio.write(bytes)\n",
    "            bio.seek(0)\n",
    "            try:\n",
    "                with tarfile.TarFile(fileobj=bio) as tf:\n",
    "                    for member in tf.getmembers():\n",
    "                        if member.isfile() and member.name.endswith(\".pdf\"):\n",
    "                            data = tf.extractfile(member).read()\n",
    "                            out.append((\n",
    "                                member.name, \n",
    "                                b\"matplotlib\" in data.lower()\n",
    "                            ))\n",
    "            except tarfile.ReadError:\n",
    "                pass\n",
    "            return out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9c6042c-917f-46ca-9722-c99e02fb97cb",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Map function across every directory\n",
    "\n",
    "Let's scale up this work across all of the directories in our dataset.\n",
    "\n",
    "Hopefully it will also be faster because the cloud VMs are in the same region as the dataset itself."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89a6ab9f-3451-48dc-abb2-f4d8f5e0b038",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "results = extract.map(directories)\n",
    "lists = list(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84af922e-4f6b-4f4b-97d5-a7097084aa1b",
   "metadata": {},
   "source": [
    "Now that we're done with the large data problem we can turn off Coiled and proceed with pure Pandas. There's no reason to deal with scalable tools if we don't have to."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "299b5097-ff28-4036-b569-a18449cca0d9",
   "metadata": {},
   "source": [
    "## Enrich Data\n",
    "\n",
    "Let's enhance our data a bit.  The filenames of each file include the year and month when they were published.  After extracting this data we'll be able to see a timeseries of Matplotlib adoption."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85d74a56-e7c1-4614-86bd-6342e16d58fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to Pandas\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "dfs = [\n",
    "    pd.DataFrame(list, columns=[\"filename\", \"has_matplotlib\"]) \n",
    "    for list in lists\n",
    "]\n",
    "\n",
    "df = pd.concat(dfs)\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f98c54e7-fc46-4180-9586-c06eac6432e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def date(filename):\n",
    "    year = int(filename.split(\"/\")[0][:2])\n",
    "    month = int(filename.split(\"/\")[0][2:4])\n",
    "    if year > 80:\n",
    "        year = 1900 + year\n",
    "    else:\n",
    "        year = 2000 + year\n",
    "    \n",
    "    return pd.Timestamp(year=year, month=month, day=1)\n",
    "\n",
    "date(\"0005/astro-ph0001322.pdf\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "033451dc-344f-40e0-bd05-f2f6fd80d0c2",
   "metadata": {},
   "source": [
    "Yup.  That seems to work.  Let's map this function over our dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "315f8de5-c53a-49d9-8f8d-3ba62fcee727",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"date\"] = df.filename.map(date)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e64781f1-0486-4eda-81b4-68911383be7a",
   "metadata": {},
   "source": [
    "## Plot\n",
    "\n",
    "Now we can just fool around with Pandas and Matplotlib."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "270ae46c-ac65-48f1-a7ca-c6f742591c7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.groupby(\"date\").has_matplotlib.mean().plot(\n",
    "    title=\"Matplotlib Usage in arXiv\", \n",
    "    ylabel=\"Fraction of papers\"\n",
    ").get_figure().savefig(\"results.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd3a2592-d153-4139-a4e2-a80f4a466c24",
   "metadata": {},
   "source": [
    "I did the plot above.  Then Thomas Caswell (matplotlib maintainer) came by and, in true form, made something much better ðŸ™‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf83b666-05c1-47f3-a2e3-55bbe8f5eeb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.ticker import PercentFormatter\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "by_month = df.groupby(\"date\").has_matplotlib.mean()\n",
    "\n",
    "# get figure\n",
    "fig, ax = plt.subplots(layout=\"constrained\")\n",
    "# plot the data\n",
    "ax.plot(by_month, \"o\", color=\"k\", ms=3)\n",
    "\n",
    "# over-ride the default auto limits\n",
    "ax.set_xlim(left=datetime.date(2004, 1, 1))\n",
    "ax.set_ylim(bottom=0)\n",
    "\n",
    "# turn on a horizontal grid\n",
    "ax.grid(axis=\"y\")\n",
    "\n",
    "# remove the top and right spines\n",
    "ax.spines.right.set_visible(False)\n",
    "ax.spines.top.set_visible(False)\n",
    "\n",
    "# format y-ticks a percent\n",
    "ax.yaxis.set_major_formatter(PercentFormatter(xmax=1))\n",
    "\n",
    "# add title and labels\n",
    "ax.set_xlabel(\"date\")\n",
    "ax.set_ylabel(\"% of all papers\")\n",
    "ax.set_title(\"Matplotlib usage on arXiv\");"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81198952-f6ba-4e1b-9792-36e54a5fe491",
   "metadata": {},
   "source": [
    "Yup.  Matplotlib is used pretty commonly on arXiv.  Go team."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a59d3a1d-289c-405c-8330-fc249e376b70",
   "metadata": {},
   "source": [
    "## Save results\n",
    "\n",
    "This data was slighly painful to procure.  Let's save the results locally for future analysis.  That way other researchers can further analyze the results without having to muck about with parallelism or cloud stuff."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ae0d6ff-4471-4a45-bbe1-6bcd8a8d72a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(\"arxiv-matplotlib.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "623becf0-a84d-4cf7-b719-883bfe60eef4",
   "metadata": {},
   "outputs": [],
   "source": [
    "!du -hs arxiv-matplotlib.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1755b7a4-8b92-441c-9083-2bc0b51e2f81",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_parquet(\"arxiv-matplotlib.parquet\", compression=\"snappy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afc412d6-38b3-424e-8ca5-b4141d1b776f",
   "metadata": {},
   "outputs": [],
   "source": [
    "!du -hs arxiv-matplotlib.parquet"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f637b60b-7051-4898-9c99-b3b436acd1ae",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "### Matplotlib + arXiv\n",
    "\n",
    "It's incredible to see the steady growth of Matplotlib across arXiv.  It's worth noting that this is *all* papers, even from fields like theoretical mathematics that are unlikely to include computer generated plots.  Is this matplotlib growing in popularity?  Is it Python generally?\n",
    "\n",
    "For future work, we should break this down by subfield.  The filenames actually contained the name of the field for a while, like \"hep-ex\" for \"high energy physics, experimental\", but it looks like arXiv stopped doing this at some point.  My guess is that there is a list mapping filenames to fields somewhere though.  The filenames are all in the Pandas dataframe / parquet dataset, so doing this analysis shouldn't require any scalable computing.\n",
    "\n",
    "### Coiled\n",
    "\n",
    "Coiled was built to make it easy to answer large questions.  \n",
    "\n",
    "We started this notebook with some generic Python code. When we wanted to scale up we invoked Coiled, did some work, and then tore things down, all in about ten minutes. The problem of scale or \"big data\" didn't get in the way of us analyzing data and making a delightful discovery. \n",
    "\n",
    "This is exactly why these projects exist."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
