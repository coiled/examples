{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f6031837-1dc6-4773-9fa4-b892b6f0e968",
   "metadata": {},
   "source": [
    "# Scaling XGBoost with Dask and Coiled\n",
    "\n",
    "[XGBoost](https://xgboost.readthedocs.io/en/latest/) is a library used for training gradient boosted supervised machine learning models, and it has a [Dask integration](https://xgboost.readthedocs.io/en/latest/tutorials/dask.html) for distributed training. In this guide, you'll learn how to train an XGBoost model in parallel using Dask and Coiled. Download {download}`this jupyter notebook <dask-xgboost.ipynb>` to follow along."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fed3d0a-193a-4b89-9a88-d114c394a89a",
   "metadata": {},
   "source": [
    "## Before you start\n",
    "\n",
    "You'll first need install the necessary packages. For the purposes of this example, we'll do this in a new virtual environment, but you could also install them in whatever environment you're already using for your project.\n",
    "\n",
    "```bash\n",
    "$ conda create -n dask-xgboost-example -c conda-forge python=3.10 dask coiled s3fs pyarrow dask-ml\n",
    "$ conda activate dask-xgboost-example\n",
    "$ pip install xgboost\n",
    "```\n",
    "  \n",
    "You also could use `pip` for everything, or any other package manager you prefer; `conda` isn't required.\n",
    "\n",
    "When you create a cluster, Coiled will automatically replicate your local `dask-xgboost-example` environment to your cluster."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fe7754e-88e1-4a6a-bb16-527f64de3d2d",
   "metadata": {},
   "source": [
    "## About the Data\n",
    "\n",
    "In this example we will use a dataset that the Coiled team created by pre-processing the\n",
    "Uber/Lyft dataset from the [High-Volume For-Hire Services](https://www.nyc.gov/site/tlc/businesses/high-volume-for-hire-services.page), joined it with the [NYC Taxi Zone Lookup Table](https://www.nyc.gov/site/tlc/about/tlc-trip-record-data.page). This results in a dataset with ~1.4 billion rows. \n",
    "\n",
    "\n",
    "## About the Model\n",
    "\n",
    "We will be training a single XGBoost model with Dask using the `xgboost.dask` module built into XGBoost. In this notebook we:\n",
    "\n",
    "- Load the data\n",
    "- Perform basic feature engineering (date type optimization, categorization)\n",
    "- Train a single model with XGBoost, using custom cross-validation\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1357a947-9ccf-43cf-928c-4e072ea88839",
   "metadata": {},
   "source": [
    "## Get a Coiled Cluster\n",
    "\n",
    "To start we need to spin up a Dask cluster.\n",
    "\n",
    ":::{note}  \n",
    "The total amount of RAM needed on the cluster is directly proportional to the total size of the training dataset. Users should expect large amounts of unmanaged memory during the training phase, consumed by the heap of the training tasks. This heap memory requirements are directly proportional to total dataset size / number of workers. This is unlike most other dask workflows, where heap size is proportional to partition size * threads per worker.  \n",
    ":::"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a588327d-b81a-4397-8ba4-55fce39126f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import coiled\n",
    "\n",
    "cluster = coiled.Cluster(\n",
    "    n_workers=50,\n",
    "    name=\"dask-xgboost\",\n",
    "    worker_vm_types=[\"r6i.large\"],\n",
    "    scheduler_vm_types=[\"m6i.large\"],\n",
    "    backend_options={\"region\": \"us-east-2\"},  # match data zone\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aee02d52-52f5-4ad8-8b5b-e24e53c0dab8",
   "metadata": {},
   "source": [
    "and then connect Dask to your remote Coiled cluster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d92bd765-1152-4ac8-a723-8fca1fb5ac45",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dask.distributed import Client\n",
    "\n",
    "client = Client(cluster)\n",
    "client"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2137fe6-a09b-46a2-bd79-f2765a5a58b1",
   "metadata": {},
   "source": [
    "## Load the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "69de5d15-e26f-49f7-8981-93ca91d4a96e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import dask.dataframe as dd\n",
    "\n",
    "ddf = dd.read_parquet(\n",
    "    \"s3://coiled-datasets/dask-xgboost-example/feature_table.parquet/\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98a11a2f-45ab-4829-8e36-40070148a983",
   "metadata": {},
   "source": [
    "## Basic Feature Engineering\n",
    "\n",
    "This dataset is pretty polished, but there are few details to take care of that are specific to the data types."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "029c7ab8-a59a-4089-a653-6b5b92f17f2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Under the hood, XGBoost converts floats to `float32`.\n",
    "# Let's do it only once here.\n",
    "float_cols = ddf.select_dtypes(include=\"float\").columns.tolist()\n",
    "ddf = ddf.astype({c: np.float32 for c in float_cols})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f5b43731-841f-4311-a782-1f786e2be639",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We need the categories to be known\n",
    "categorical_vars = ddf.select_dtypes(include=\"category\").columns.tolist()\n",
    "\n",
    "# categorize() reads the whole input and then discards it.\n",
    "# Let's read from disk only once.\n",
    "ddf = ddf.persist()\n",
    "ddf = ddf.categorize(columns=categorical_vars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b758f837-c842-43a1-bb50-53d5935e2a13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We will need to access this multiple times. Let's persist it.\n",
    "ddf = ddf.persist()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcaf580c-4f7c-4564-a35b-9f1ce94cc3bd",
   "metadata": {},
   "source": [
    "## Custom cross-validation\n",
    "\n",
    "In this example we show you how you can use a custom cross-validation function such as:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2cf4eff6-0174-45b2-9059-3e061e88c2e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of folds determines the train/test split\n",
    "# (e.g. N_FOLDS=5 -> train=4/5 of the total data, test=1/5)\n",
    "N_FOLDS = 5\n",
    "\n",
    "\n",
    "def make_cv_splits(n_folds = N_FOLDS):\n",
    "    frac = [1 / n_folds] * n_folds\n",
    "    splits = ddf.random_split(frac, shuffle=True)\n",
    "    for i in range(n_folds):\n",
    "        train = [splits[j] for j in range(n_folds) if j != i]\n",
    "        test = splits[i]\n",
    "        yield dd.concat(train), test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "687b2949-2252-4ad4-ad34-a2e80c255dc3",
   "metadata": {},
   "source": [
    "## Train Model\n",
    "\n",
    "When using XGBoost with Dask, we need to call the XGBoost Dask interface from the client side. The main difference with XGBoostâ€™s Dask interface is that we pass our Dask client as an additional argument for carrying out the computation. Note that if the `client` parameter below is set to `None`, XGBoost will use the default client returned by Dask.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "55a95362-92ab-4296-9bb1-9874a9eec4ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training/Test split #0\n",
      "Building DMatrix...\n",
      "Training model...\n",
      "Running model on test data...\n",
      "Measuring accuracy of model vs. ground truth...\n",
      "--------------------------------------------------------------------------------\n",
      "Training/Test split #1\n",
      "Building DMatrix...\n",
      "Training model...\n",
      "Running model on test data...\n",
      "Measuring accuracy of model vs. ground truth...\n",
      "--------------------------------------------------------------------------------\n",
      "Training/Test split #2\n",
      "Building DMatrix...\n",
      "Training model...\n",
      "Running model on test data...\n",
      "Measuring accuracy of model vs. ground truth...\n",
      "--------------------------------------------------------------------------------\n",
      "Training/Test split #3\n",
      "Building DMatrix...\n",
      "Training model...\n",
      "Running model on test data...\n",
      "Measuring accuracy of model vs. ground truth...\n",
      "--------------------------------------------------------------------------------\n",
      "Training/Test split #4\n",
      "Building DMatrix...\n",
      "Training model...\n",
      "Running model on test data...\n",
      "Measuring accuracy of model vs. ground truth...\n",
      "--------------------------------------------------------------------------------\n",
      "RSME=452.76386874699745 +/- 0.31916762874476584\n",
      "Total time:  0:07:43.182956\n"
     ]
    }
   ],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "import dask.array as da\n",
    "import xgboost\n",
    "from dask_ml.metrics import mean_squared_error\n",
    "\n",
    "start = datetime.now()\n",
    "scores = []\n",
    "\n",
    "for i, (train, test) in enumerate(make_cv_splits()):\n",
    "    print(f\"Training/Test split #{i}\")\n",
    "    y_train = train[\"trip_time\"]\n",
    "    X_train = train.drop(columns=[\"trip_time\"])\n",
    "    y_test = test[\"trip_time\"]\n",
    "    X_test = test.drop(columns=[\"trip_time\"])\n",
    "\n",
    "    print(\"Building DMatrix...\")\n",
    "    d_train = xgboost.dask.DaskDMatrix(None, X_train, y_train, enable_categorical=True)\n",
    "\n",
    "    print(\"Training model...\")\n",
    "    model = xgboost.dask.train(\n",
    "        None,\n",
    "        {\"tree_method\": \"hist\"},\n",
    "        d_train,\n",
    "        num_boost_round=4,\n",
    "        evals=[(d_train, \"train\")],\n",
    "    )\n",
    "\n",
    "    print(\"Running model on test data...\")\n",
    "    predictions = xgboost.dask.predict(None, model, X_test)\n",
    "\n",
    "    print(\"Measuring accuracy of model vs. ground truth...\")\n",
    "    score = mean_squared_error(\n",
    "        y_test.to_dask_array(),\n",
    "        predictions.to_dask_array(),\n",
    "        squared=False,\n",
    "        compute=False,\n",
    "    )\n",
    "    # Compute predictions and mean squared error for this iteration\n",
    "    # while we start the next one\n",
    "    scores.append(score.reshape(1).persist())\n",
    "    print(\"-\" * 80)\n",
    "\n",
    "scores = da.concatenate(scores).compute()\n",
    "print(f\"RSME={scores.mean()} +/- {scores.std()}\")\n",
    "print(f\"Total time:  {datetime.now() - start}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4906d315-2e93-4c24-8b23-98fcebd3659a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'booster': <xgboost.core.Booster at 0x284455de0>,\n",
       " 'history': {'train': OrderedDict([('rmse',\n",
       "                [942.7679525904991,\n",
       "                 704.6123860747031,\n",
       "                 549.255274388161,\n",
       "                 451.5426385498415])])}}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Inspect our model\n",
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48292f1a-09dd-443c-ad9b-1393aee0a510",
   "metadata": {},
   "source": [
    "## Clean up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "eb9ee713-fab6-4660-b034-34023fb4db9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
